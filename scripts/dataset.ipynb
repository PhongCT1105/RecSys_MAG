{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8b428ca",
   "metadata": {},
   "source": [
    "### I. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef7fcdf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42000, 11), (9000, 11), (9000, 11))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json, ast\n",
    "import pandas as pd\n",
    "\n",
    "ROOT = Path(\".\").resolve()          # your notebook folder\n",
    "PATH_TRAIN = ROOT / \"dataset\" / \"train.txt\"\n",
    "PATH_VAL   = ROOT / \"dataset\" / \"val.txt\"\n",
    "PATH_TEST  = ROOT / \"dataset\" / \"test.txt\"\n",
    "\n",
    "PATH_TRAIN, PATH_VAL, PATH_TEST\n",
    "\n",
    "def load_split_whole_file(path: Path) -> pd.DataFrame:\n",
    "    # read the whole file into one string\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # try JSON first\n",
    "    try:\n",
    "        obj = json.loads(content)\n",
    "    except json.JSONDecodeError:\n",
    "        # if it's not valid JSON, treat it as a Python literal\n",
    "        obj = ast.literal_eval(content)\n",
    "\n",
    "    # we expect a *list of dicts*\n",
    "    if not isinstance(obj, list):\n",
    "        raise TypeError(f\"Expected a list, got {type(obj)} from {path}\")\n",
    "    \n",
    "    # one row per paper\n",
    "    df = pd.json_normalize(obj)   # or pd.DataFrame(obj)\n",
    "    return df\n",
    "\n",
    "df_train = load_split_whole_file(PATH_TRAIN)\n",
    "df_val   = load_split_whole_file(PATH_VAL)\n",
    "df_test  = load_split_whole_file(PATH_TEST)\n",
    "\n",
    "df_train.shape, df_val.shape, df_test.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54c22662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 12),\n",
       " split\n",
       " train    42000\n",
       " val       9000\n",
       " test      9000\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"split\"] = \"train\"\n",
    "df_val[\"split\"]   = \"val\"\n",
    "df_test[\"split\"]  = \"test\"\n",
    "\n",
    "df_raw = pd.concat([df_train, df_val, df_test], ignore_index=True)\n",
    "df_raw.shape, df_raw[\"split\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c7a0b525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publication_ID</th>\n",
       "      <th>Citations</th>\n",
       "      <th>pubDate</th>\n",
       "      <th>language</th>\n",
       "      <th>title</th>\n",
       "      <th>journal</th>\n",
       "      <th>abstract</th>\n",
       "      <th>keywords</th>\n",
       "      <th>authors</th>\n",
       "      <th>venue</th>\n",
       "      <th>doi</th>\n",
       "      <th>split</th>\n",
       "      <th>citation_list</th>\n",
       "      <th>citation_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17396995</td>\n",
       "      <td>17957262;21818356;24164861;21818356;24164861;2...</td>\n",
       "      <td>2007 May 1</td>\n",
       "      <td>eng</td>\n",
       "      <td>Herpes simplex virus type 2 infection does not...</td>\n",
       "      <td>The Journal of infectious diseases</td>\n",
       "      <td>We sought to compare baseline and longitudinal...</td>\n",
       "      <td>Adult;California;epidemiology;Cohort Studies;H...</td>\n",
       "      <td>[{'name': 'Edward R Cachay', 'org': 'Universit...</td>\n",
       "      <td>{'name': 'The Journal of infectious diseases'}</td>\n",
       "      <td>10.1086/513568</td>\n",
       "      <td>train</td>\n",
       "      <td>[17957262, 21818356, 24164861, 21818356, 24164...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16779733</td>\n",
       "      <td>19197361;19399183;20041174;20300572;17311474;2...</td>\n",
       "      <td>2006 Jul 15</td>\n",
       "      <td>eng</td>\n",
       "      <td>Efficacy of the anti Candida rAls3p N or rAls1...</td>\n",
       "      <td>The Journal of infectious diseases.</td>\n",
       "      <td>We have shown that vaccination with the recomb...</td>\n",
       "      <td>Animals;Candida;immunology;isolation &amp; purific...</td>\n",
       "      <td>[{'name': 'Brad J Spellberg', 'org': 'Departme...</td>\n",
       "      <td>{'name': 'The Journal of infectious diseases'}</td>\n",
       "      <td>10.1086/504691</td>\n",
       "      <td>train</td>\n",
       "      <td>[19197361, 19399183, 20041174, 20300572, 17311...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12412787</td>\n",
       "      <td>28740334</td>\n",
       "      <td>2002 Nov</td>\n",
       "      <td>eng</td>\n",
       "      <td>Role of the interleukin 6 interleukin 6 solubl...</td>\n",
       "      <td>Journal of bone and mineral research : the off...</td>\n",
       "      <td>We have observed a strong correlation between ...</td>\n",
       "      <td>Adult;Animals;Bone Resorption;metabolism;Bone ...</td>\n",
       "      <td>[{'name': 'Karl Insogna', 'org': 'Department o...</td>\n",
       "      <td>{'name': 'Journal of bone and mineral research...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>[28740334]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18070707</td>\n",
       "      <td>22567368;22348393;22495885;23874387;23100393;2...</td>\n",
       "      <td>2007 Dec</td>\n",
       "      <td>eng</td>\n",
       "      <td>Genetic events in the pathogenesis of multiple...</td>\n",
       "      <td>Best practice &amp; research. Clinical haematology</td>\n",
       "      <td>The genetics of myeloma has been increasingly ...</td>\n",
       "      <td>Gene Expression Profiling;Humans;Immunoglobuli...</td>\n",
       "      <td>[{'name': 'W.J. Chng', 'org': 'Mayo Clinic Ari...</td>\n",
       "      <td>{'name': 'Best practice &amp; research. Clinical h...</td>\n",
       "      <td>10.1016/j.beha.2007.08.004</td>\n",
       "      <td>train</td>\n",
       "      <td>[22567368, 22348393, 22495885, 23874387, 23100...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16365419</td>\n",
       "      <td>20498830;26334995</td>\n",
       "      <td>2006 Jan 01</td>\n",
       "      <td>eng</td>\n",
       "      <td>PU 1 regulates cathepsin S expression in profe...</td>\n",
       "      <td>Journal of immunology (Baltimore, Md. : 1950)</td>\n",
       "      <td>Cathepsin S (CTSS) is a cysteine protease that...</td>\n",
       "      <td>Animals;Antigen-Presenting Cells;immunology;me...</td>\n",
       "      <td>[{'name': 'Ying Wang', 'id': '53f5626bdabfae5d...</td>\n",
       "      <td>{'name': 'Journal of immunology (Baltimore, Md...</td>\n",
       "      <td>10.4049/jimmunol.176.1.275</td>\n",
       "      <td>train</td>\n",
       "      <td>[20498830, 26334995]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publication_ID                                          Citations  \\\n",
       "0        17396995  17957262;21818356;24164861;21818356;24164861;2...   \n",
       "1        16779733  19197361;19399183;20041174;20300572;17311474;2...   \n",
       "2        12412787                                           28740334   \n",
       "3        18070707  22567368;22348393;22495885;23874387;23100393;2...   \n",
       "4        16365419                                  20498830;26334995   \n",
       "\n",
       "       pubDate language                                              title  \\\n",
       "0   2007 May 1      eng  Herpes simplex virus type 2 infection does not...   \n",
       "1  2006 Jul 15      eng  Efficacy of the anti Candida rAls3p N or rAls1...   \n",
       "2     2002 Nov      eng  Role of the interleukin 6 interleukin 6 solubl...   \n",
       "3     2007 Dec      eng  Genetic events in the pathogenesis of multiple...   \n",
       "4  2006 Jan 01      eng  PU 1 regulates cathepsin S expression in profe...   \n",
       "\n",
       "                                             journal  \\\n",
       "0                 The Journal of infectious diseases   \n",
       "1                The Journal of infectious diseases.   \n",
       "2  Journal of bone and mineral research : the off...   \n",
       "3     Best practice & research. Clinical haematology   \n",
       "4      Journal of immunology (Baltimore, Md. : 1950)   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  We sought to compare baseline and longitudinal...   \n",
       "1  We have shown that vaccination with the recomb...   \n",
       "2  We have observed a strong correlation between ...   \n",
       "3  The genetics of myeloma has been increasingly ...   \n",
       "4  Cathepsin S (CTSS) is a cysteine protease that...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  Adult;California;epidemiology;Cohort Studies;H...   \n",
       "1  Animals;Candida;immunology;isolation & purific...   \n",
       "2  Adult;Animals;Bone Resorption;metabolism;Bone ...   \n",
       "3  Gene Expression Profiling;Humans;Immunoglobuli...   \n",
       "4  Animals;Antigen-Presenting Cells;immunology;me...   \n",
       "\n",
       "                                             authors  \\\n",
       "0  [{'name': 'Edward R Cachay', 'org': 'Universit...   \n",
       "1  [{'name': 'Brad J Spellberg', 'org': 'Departme...   \n",
       "2  [{'name': 'Karl Insogna', 'org': 'Department o...   \n",
       "3  [{'name': 'W.J. Chng', 'org': 'Mayo Clinic Ari...   \n",
       "4  [{'name': 'Ying Wang', 'id': '53f5626bdabfae5d...   \n",
       "\n",
       "                                               venue  \\\n",
       "0     {'name': 'The Journal of infectious diseases'}   \n",
       "1     {'name': 'The Journal of infectious diseases'}   \n",
       "2  {'name': 'Journal of bone and mineral research...   \n",
       "3  {'name': 'Best practice & research. Clinical h...   \n",
       "4  {'name': 'Journal of immunology (Baltimore, Md...   \n",
       "\n",
       "                          doi  split  \\\n",
       "0              10.1086/513568  train   \n",
       "1              10.1086/504691  train   \n",
       "2                           0  train   \n",
       "3  10.1016/j.beha.2007.08.004  train   \n",
       "4  10.4049/jimmunol.176.1.275  train   \n",
       "\n",
       "                                       citation_list  citation_count  \n",
       "0  [17957262, 21818356, 24164861, 21818356, 24164...               7  \n",
       "1  [19197361, 19399183, 20041174, 20300572, 17311...              21  \n",
       "2                                         [28740334]               1  \n",
       "3  [22567368, 22348393, 22495885, 23874387, 23100...              18  \n",
       "4                               [20498830, 26334995]               2  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_papers(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # keep only non-empty title & abstract\n",
    "    df = df[df[\"title\"].notna() & df[\"abstract\"].notna()]\n",
    "    df = df[df[\"abstract\"].str.len() > 20]\n",
    "\n",
    "    # parse \"179;238;...\" → [179, 238, ...]\n",
    "    def parse_citations(cstr):\n",
    "        if not isinstance(cstr, str):\n",
    "            return []\n",
    "        out = []\n",
    "        for c in cstr.split(\";\"):\n",
    "            c = c.strip()\n",
    "            if c.isdigit():\n",
    "                out.append(int(c))\n",
    "        return out\n",
    "\n",
    "    df[\"citation_list\"]  = df[\"Citations\"].apply(parse_citations)\n",
    "    df[\"citation_count\"] = df[\"citation_list\"].str.len()\n",
    "\n",
    "    # optional: filter\n",
    "    df = df[df[\"citation_count\"] > 0]\n",
    "    df[\"publication_ID\"] = df[\"publication_ID\"].astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = preprocess_papers(df_raw)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "946f795e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56416"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_ids = sorted(df[\"publication_ID\"].unique())\n",
    "paper_id2idx = {pid: i for i, pid in enumerate(paper_ids)}\n",
    "idx2paper_id = {i: pid for pid, i in paper_id2idx.items()}\n",
    "num_nodes = len(paper_ids)\n",
    "num_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f26108ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6789, 1481, 1446)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_set = set(paper_ids)\n",
    "\n",
    "def build_edges_for_split(df_split):\n",
    "    src_list, dst_list = [], []\n",
    "    for _, row in df_split.iterrows():\n",
    "        src_pid = int(row[\"publication_ID\"])\n",
    "        if src_pid not in paper_id2idx:\n",
    "            continue\n",
    "        src_idx = paper_id2idx[src_pid]\n",
    "        for cited_pid in row[\"citation_list\"]:\n",
    "            if cited_pid in paper_set:\n",
    "                dst_idx = paper_id2idx[cited_pid]\n",
    "                src_list.append(src_idx)\n",
    "                dst_list.append(dst_idx)\n",
    "    return src_list, dst_list\n",
    "\n",
    "df_tr = df[df[\"split\"] == \"train\"]\n",
    "df_va = df[df[\"split\"] == \"val\"]\n",
    "df_te = df[df[\"split\"] == \"test\"]\n",
    "\n",
    "train_src, train_dst = build_edges_for_split(df_tr)\n",
    "val_src,   val_dst   = build_edges_for_split(df_va)\n",
    "test_src,  test_dst  = build_edges_for_split(df_te)\n",
    "\n",
    "len(train_src), len(val_src), len(test_src)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06fa50d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/phongcao/Work/RecSys_MAG/.venv/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Batches: 100%|██████████| 910/910 [04:14<00:00,  3.57it/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\") \n",
    "\n",
    "texts = (df[\"title\"].fillna(\"\") + \" \" + df[\"abstract\"].fillna(\"\")).tolist()\n",
    "emb = model.encode(texts, batch_size=64, show_progress_bar=True)  # (N_df, d)\n",
    "\n",
    "emb_dim = emb.shape[1]\n",
    "x = np.zeros((num_nodes, emb_dim), dtype=\"float32\")\n",
    "\n",
    "# put embeddings into the correct row for each node index\n",
    "for i, (_, row) in enumerate(df.iterrows()):\n",
    "    pid = int(row[\"publication_ID\"])\n",
    "    idx = paper_id2idx[pid]\n",
    "    x[idx] = emb[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d191f7aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[56416, 384], edge_index=[2, 6789], train_pos_edge_index=[2, 6789], val_pos_edge_index=[2, 1481], test_pos_edge_index=[2, 1446])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "x_tensor = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "edge_index_train = torch.tensor([train_src, train_dst], dtype=torch.long)\n",
    "edge_index_val   = torch.tensor([val_src,   val_dst],   dtype=torch.long)\n",
    "edge_index_test  = torch.tensor([test_src,  test_dst],  dtype=torch.long)\n",
    "\n",
    "data = Data(x=x_tensor, edge_index=edge_index_train)\n",
    "data.train_pos_edge_index = edge_index_train\n",
    "data.val_pos_edge_index   = edge_index_val\n",
    "data.test_pos_edge_index  = edge_index_test\n",
    "\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fa668019",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import SAGEConv\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Module\n",
    "\n",
    "class GraphSAGEEncoder(Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_dim, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class LinkPredictor(Module):\n",
    "    def __init__(self, in_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.lin1 = Linear(in_dim * 2, hidden_dim)\n",
    "        self.lin2 = Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, z_src, z_dst):\n",
    "        h = torch.cat([z_src, z_dst], dim=-1)\n",
    "        h = F.relu(self.lin1(h))\n",
    "        h = self.lin2(h)\n",
    "        return torch.sigmoid(h).view(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "94b0b5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "in_dim = data.x.size(1)\n",
    "hidden_dim = 256\n",
    "out_dim = 256   # embedding size for papers\n",
    "\n",
    "encoder = GraphSAGEEncoder(in_dim, hidden_dim, out_dim).to(device)\n",
    "predictor = LinkPredictor(out_dim).to(device)\n",
    "\n",
    "data = data.to(device)\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(encoder.parameters()) + list(predictor.parameters()),\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4d61acff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def negative_sampling(num_nodes, pos_edge_index, num_neg_samples=None):\n",
    "    \"\"\"\n",
    "    Very simple uniform negative sampling.\n",
    "    Returns [2, num_neg_samples].\n",
    "    \"\"\"\n",
    "    if num_neg_samples is None:\n",
    "        num_neg_samples = pos_edge_index.size(1)\n",
    "\n",
    "    # set of existing positive edges to avoid sampling them as negatives\n",
    "    pos = set(\n",
    "        (int(s), int(d))\n",
    "        for s, d in zip(pos_edge_index[0].tolist(), pos_edge_index[1].tolist())\n",
    "    )\n",
    "\n",
    "    neg_src = []\n",
    "    neg_dst = []\n",
    "\n",
    "    # brute-force is OK for moderate graphs (your subset)\n",
    "    while len(neg_src) < num_neg_samples:\n",
    "        src = torch.randint(0, num_nodes, (1,)).item()\n",
    "        dst = torch.randint(0, num_nodes, (1,)).item()\n",
    "        if (src, dst) in pos:\n",
    "            continue\n",
    "        neg_src.append(src)\n",
    "        neg_dst.append(dst)\n",
    "\n",
    "    neg_edge_index = torch.tensor([neg_src, neg_dst], dtype=torch.long, device=pos_edge_index.device)\n",
    "    return neg_edge_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ed199b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(data):\n",
    "    encoder.train()\n",
    "    predictor.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 1. Node embeddings from GraphSAGE\n",
    "    z = encoder(data.x, data.edge_index)  # [N, out_dim]\n",
    "\n",
    "    pos_edge_index = data.train_pos_edge_index  # [2, E_train]\n",
    "    num_nodes = data.num_nodes\n",
    "\n",
    "    # 2. Positive samples\n",
    "    src_pos = pos_edge_index[0]\n",
    "    dst_pos = pos_edge_index[1]\n",
    "    z_src_pos = z[src_pos]\n",
    "    z_dst_pos = z[dst_pos]\n",
    "    pred_pos = predictor(z_src_pos, z_dst_pos)\n",
    "\n",
    "    # 3. Negative samples (same number as positives)\n",
    "    neg_edge_index = negative_sampling(num_nodes, pos_edge_index,\n",
    "                                       num_neg_samples=pos_edge_index.size(1))\n",
    "    src_neg = neg_edge_index[0]\n",
    "    dst_neg = neg_edge_index[1]\n",
    "    z_src_neg = z[src_neg]\n",
    "    z_dst_neg = z[dst_neg]\n",
    "    pred_neg = predictor(z_src_neg, z_dst_neg)\n",
    "\n",
    "    # 4. Loss: encourage high scores for pos, low for neg\n",
    "    loss_pos = -torch.log(pred_pos + 1e-15).mean()\n",
    "    loss_neg = -torch.log(1 - pred_neg + 1e-15).mean()\n",
    "    loss = loss_pos + loss_neg\n",
    "\n",
    "    # 5. Backprop once\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return float(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e7f50204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(z, pos_edge_index, k=10, num_neg_candidates=99):\n",
    "    num_nodes = z.size(0)\n",
    "    pos_src = pos_edge_index[0]\n",
    "    pos_dst = pos_edge_index[1]\n",
    "    num_pos = pos_edge_index.size(1)\n",
    "\n",
    "    hits = 0\n",
    "\n",
    "    for i in range(num_pos):\n",
    "        src = int(pos_src[i])\n",
    "        true_dst = int(pos_dst[i])\n",
    "\n",
    "        # sample negatives\n",
    "        neg_dst = []\n",
    "        while len(neg_dst) < num_neg_candidates:\n",
    "            cand = torch.randint(0, num_nodes, (1,)).item()\n",
    "            if cand == true_dst:\n",
    "                continue\n",
    "            neg_dst.append(cand)\n",
    "\n",
    "        candidates = torch.tensor([true_dst] + neg_dst, device=z.device)\n",
    "        src_batch = torch.full_like(candidates, src)\n",
    "\n",
    "        scores = predictor(z[src_batch], z[candidates])  # [num_candidates]\n",
    "\n",
    "        # higher score = more likely citation\n",
    "        topk = torch.topk(scores, k=min(k, len(scores)), largest=True).indices\n",
    "        # index 0 in `candidates` corresponds to true_dst\n",
    "        if 0 in topk:\n",
    "            hits += 1\n",
    "\n",
    "    return hits / num_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e0339d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(data, k=10):\n",
    "    encoder.eval()\n",
    "    predictor.eval()\n",
    "\n",
    "    z = encoder(data.x, data.edge_index)\n",
    "\n",
    "    val_recall = recall_at_k(z, data.val_pos_edge_index, k=k)\n",
    "    test_recall = recall_at_k(z, data.test_pos_edge_index, k=k)\n",
    "\n",
    "    return val_recall, test_recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ec6f3fc9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m best_state = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, num_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     loss = \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4096\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     val_recall, test_recall = evaluate(data, k=\u001b[32m10\u001b[39m)\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m val_recall > best_val:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mtrain_step\u001b[39m\u001b[34m(data, batch_size)\u001b[39m\n\u001b[32m     41\u001b[39m loss_neg = -torch.log(\u001b[32m1\u001b[39m - pred_neg + \u001b[32m1e-15\u001b[39m).mean()\n\u001b[32m     42\u001b[39m loss = loss_pos + loss_neg\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m total_loss += \u001b[38;5;28mfloat\u001b[39m(loss.item())\n\u001b[32m     46\u001b[39m num_batches += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/RecSys_MAG/.venv/lib/python3.14/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/RecSys_MAG/.venv/lib/python3.14/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Work/RecSys_MAG/.venv/lib/python3.14/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "best_val = 0.0\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    loss = train_step(data, batch_size=4096)\n",
    "    val_recall, test_recall = evaluate(data, k=10)\n",
    "\n",
    "    if val_recall > best_val:\n",
    "        best_val = val_recall\n",
    "        best_state = {\n",
    "            \"encoder\": encoder.state_dict(),\n",
    "            \"predictor\": predictor.state_dict(),\n",
    "        }\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} | \"\n",
    "        f\"loss = {loss:.4f} | \"\n",
    "        f\"val@10 = {val_recall:.4f} | \"\n",
    "        f\"test@10 = {test_recall:.4f}\"\n",
    "    )\n",
    "\n",
    "# Load best model (according to validation Recall@K)\n",
    "if best_state is not None:\n",
    "    encoder.load_state_dict(best_state[\"encoder\"])\n",
    "    predictor.load_state_dict(best_state[\"predictor\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
